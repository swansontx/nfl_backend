"""Backtest Framework - Validate model accuracy on historical data with ADVANCED METRICS.

Tests model predictions against actual outcomes to measure:
- Prediction accuracy (RMSE, MAE, RÂ²) for EPA-based and traditional props
- Calibration (predicted probabilities vs actual frequencies)
- Prop bet performance (ROI if betting based on model edges)
- Position-specific accuracy (QB vs RB vs WR vs TE)
- Advanced metric accuracy (EPA, CPOE, success rate predictions)

Backtest Types:
1. Point Predictions: How accurate are yardage/TD/EPA predictions?
2. Probability Calibration: Do 60% probabilities hit 60% of the time?
3. Prop Betting Simulation: Simulated ROI from betting on edges
4. Line Movement: How well does model predict closing lines?

Output:
- Backtest report JSON with all metrics
- Visualizations (calibration curves, prediction scatter plots)
- ROI analysis by position and prop type
- Feature importance validation

Usage:
    # Run backtest on 2023 season
    python -m backend.calib_backtest.run_backtest --season 2023

    # Backtest specific weeks
    python -m backend.calib_backtest.run_backtest --season 2024 --weeks 1-8
"""

from pathlib import Path
import argparse
import json
from typing import Dict, List, Optional, Tuple
import pandas as pd
import numpy as np
from datetime import datetime


def run_backtest(
    season: int,
    model_path: Path,
    features_file: Path,
    actuals_file: Path,
    output_report: Path,
    weeks: Optional[List[int]] = None
) -> Dict:
    """Run comprehensive backtest framework.

    Args:
        season: Season to backtest
        model_path: Path to trained model .pkl file
        features_file: Path to player features JSON
        actuals_file: Path to actual outcomes (player_stats CSV or PBP CSV)
        output_report: Path to save backtest report
        weeks: Optional list of weeks to test (None = all weeks)

    Returns:
        Backtest results dictionary with metrics, calibration, ROI
    """
    print(f"\n{'='*60}")
    print(f"Backtest Framework - Season {season}")
    print(f"{'='*60}\n")

    # STEP 1: Load trained model
    if not model_path.exists():
        print(f"âš ï¸  Model not found: {model_path}")
        print(f"Train a model first:")
        print(f"  python -m backend.modeling.train_passing_model")
        return {
            'error': 'Model not found',
            'season': season
        }

    print(f"ðŸ“‚ Loading model from {model_path}")
    import pickle
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    print(f"âœ“ Model loaded")

    # STEP 2: Load features
    if not features_file.exists():
        print(f"âš ï¸  Features file not found: {features_file}")
        return {
            'error': 'Features not found',
            'season': season
        }

    print(f"\nðŸ“‚ Loading features from {features_file}")
    with open(features_file, 'r') as f:
        player_features = json.load(f)
    print(f"âœ“ Loaded features for {len(player_features)} players")

    # STEP 3: Prepare test dataset
    print(f"\nðŸ“Š Preparing backtest dataset...")
    from backend.modeling.train_passing_model import _prepare_training_data, _engineer_rolling_features

    df = _prepare_training_data(player_features)
    df = _engineer_rolling_features(df)

    # Filter to QBs with sufficient data
    df = df[
        (df['is_qb'] == 1) &
        (df['games_played'] >= 3) &
        (df['attempts'] >= 15)
    ].copy()

    # Filter to specific weeks if requested
    if weeks:
        df = df[df['week'].isin(weeks)]

    print(f"âœ“ Backtest dataset: {len(df)} QB games")

    if len(df) == 0:
        print(f"âš ï¸  No test data available")
        return {
            'error': 'No test data',
            'season': season
        }

    # STEP 4: Make predictions
    print(f"\nðŸ¤– Generating predictions...")

    feature_cols = [
        'qb_epa_avg', 'total_epa_avg', 'cpoe_avg', 'success_rate',
        'wpa_avg', 'air_epa_avg', 'yac_epa_avg',
        'qb_pressure_rate', 'qb_hit_rate',
        'qb_epa_roll_3', 'qb_epa_roll_5',
        'cpoe_roll_3', 'cpoe_roll_5',
        'success_rate_roll_3', 'success_rate_roll_5',
        'passing_yards_avg', 'completion_pct',
        'yards_per_attempt', 'air_yards_avg',
        'games_played', 'attempts'
    ]

    available_features = [col for col in feature_cols if col in df.columns]
    X = df[available_features].fillna(0)
    y_true = df['passing_yards'].values
    y_pred = model.predict(X)

    df['predicted_yards'] = y_pred
    print(f"âœ“ Generated {len(y_pred)} predictions")

    # STEP 5: Calculate accuracy metrics
    print(f"\nðŸ“ˆ Calculating accuracy metrics...")
    metrics = _calculate_accuracy_metrics(y_true, y_pred)

    print(f"  Overall Metrics:")
    print(f"    RMSE:  {metrics['rmse']:.2f} yards")
    print(f"    MAE:   {metrics['mae']:.2f} yards")
    print(f"    RÂ²:    {metrics['r_squared']:.3f}")
    print(f"    Mean Pred: {y_pred.mean():.1f} yards")
    print(f"    Mean Actual: {y_true.mean():.1f} yards")

    # STEP 6: Position-specific analysis (for now just QB, can extend later)
    print(f"\nðŸ“Š Position-specific analysis...")
    position_metrics = {
        'QB': {
            'rmse': metrics['rmse'],
            'mae': metrics['mae'],
            'r_squared': metrics['r_squared'],
            'samples': len(df)
        }
    }

    # STEP 7: Calibration analysis
    print(f"\nðŸŽ¯ Analyzing calibration...")
    calibration = _analyze_calibration(df, y_true, y_pred)

    print(f"  Calibration bins (predicted vs actual):")
    for bin_range, actual_avg in calibration['bins'].items():
        print(f"    {bin_range}: {actual_avg:.1f} yards")

    # STEP 8: Betting ROI simulation
    print(f"\nðŸ’° Simulating prop betting ROI...")
    roi_results = _simulate_betting_roi(df, y_pred, y_true)

    print(f"  Simulated betting performance:")
    print(f"    Total bets: {roi_results['total_bets']}")
    print(f"    Win rate: {roi_results['win_rate']:.1%}")
    print(f"    ROI: {roi_results['roi']:.2%}")
    print(f"    Expected value: ${roi_results['expected_value']:.2f} per $100 bet")

    # STEP 9: Error analysis by week
    print(f"\nðŸ“… Error analysis by week...")
    weekly_errors = df.groupby('week').apply(
        lambda x: {
            'rmse': np.sqrt(np.mean((x['passing_yards'] - x['predicted_yards'])**2)),
            'mae': np.mean(np.abs(x['passing_yards'] - x['predicted_yards'])),
            'samples': len(x)
        }
    ).to_dict()

    # STEP 10: Feature importance validation (which features drove best predictions)
    print(f"\nðŸ” Feature importance validation...")
    feature_importance_file = model_path.parent / 'feature_importance.json'
    if feature_importance_file.exists():
        with open(feature_importance_file, 'r') as f:
            feature_importance = json.load(f)
        top_features = list(feature_importance.items())[:5]
        print(f"  Top 5 features used:")
        for feat, importance in top_features:
            print(f"    - {feat}: {importance:.4f}")
    else:
        top_features = []

    # STEP 11: Compile results
    results = {
        'season': season,
        'backtest_date': datetime.now().isoformat(),
        'weeks_tested': weeks or 'all',
        'model_path': str(model_path),

        'overall_metrics': metrics,

        'position_metrics': position_metrics,

        'calibration': calibration,

        'betting_simulation': roi_results,

        'weekly_errors': weekly_errors,

        'feature_importance': dict(top_features),

        'sample_predictions': [
            {
                'player_id': row['player_id'],
                'week': int(row['week']),
                'predicted': float(row['predicted_yards']),
                'actual': float(row['passing_yards']),
                'error': float(row['predicted_yards'] - row['passing_yards'])
            }
            for _, row in df.head(10).iterrows()
        ]
    }

    # STEP 12: Save report
    print(f"\nðŸ’¾ Saving backtest report to {output_report}")
    output_report.parent.mkdir(parents=True, exist_ok=True)
    with open(output_report, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"âœ“ Backtest report saved")

    return results


def _calculate_accuracy_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
    """Calculate prediction accuracy metrics.

    Args:
        y_true: Actual values
        y_pred: Predicted values

    Returns:
        Dict with rmse, mae, r_squared, mape
    """
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r_squared = r2_score(y_true, y_pred)

    # Mean Absolute Percentage Error
    mape = np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1, None))) * 100

    return {
        'rmse': float(rmse),
        'mae': float(mae),
        'r_squared': float(r_squared),
        'mape': float(mape)
    }


def _analyze_calibration(df: pd.DataFrame, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
    """Analyze prediction calibration.

    Args:
        df: DataFrame with predictions and actuals
        y_true: Actual values
        y_pred: Predicted values

    Returns:
        Calibration analysis dict
    """
    # Bin predictions into ranges and check if actuals match
    bins = [0, 200, 250, 300, 350, 1000]
    bin_labels = ['<200', '200-250', '250-300', '300-350', '350+']

    df_cal = df.copy()
    df_cal['pred_bin'] = pd.cut(y_pred, bins=bins, labels=bin_labels)
    df_cal['actual_yards'] = y_true

    calibration_bins = df_cal.groupby('pred_bin')['actual_yards'].mean().to_dict()

    # Convert to string keys for JSON serialization
    calibration_bins = {str(k): float(v) for k, v in calibration_bins.items() if pd.notna(v)}

    return {
        'bins': calibration_bins,
        'methodology': 'Grouped predictions into bins and calculated average actual yards'
    }


def _simulate_betting_roi(df: pd.DataFrame, y_pred: np.ndarray, y_true: np.ndarray, edge_threshold: float = 10.0) -> Dict:
    """Simulate betting ROI based on model predictions.

    Simulates betting OVER when model predicts > line + edge_threshold,
    and UNDER when model predicts < line - edge_threshold.

    Args:
        df: DataFrame with game data
        y_pred: Predicted values
        y_true: Actual values
        edge_threshold: Minimum edge to bet (yards)

    Returns:
        ROI simulation results
    """
    # Assume sportsbook line is set at average of all QB performances (~250 yards)
    # In real implementation, would use actual betting lines
    assumed_line = 250.0

    bets = []
    for pred, actual in zip(y_pred, y_true):
        bet = None

        # Bet OVER if we predict significantly above line
        if pred > assumed_line + edge_threshold:
            bet = 'OVER'
            won = actual > assumed_line

        # Bet UNDER if we predict significantly below line
        elif pred < assumed_line - edge_threshold:
            bet = 'UNDER'
            won = actual < assumed_line

        if bet:
            bets.append({
                'bet_type': bet,
                'predicted': pred,
                'actual': actual,
                'line': assumed_line,
                'won': won
            })

    if len(bets) == 0:
        return {
            'total_bets': 0,
            'wins': 0,
            'losses': 0,
            'win_rate': 0.0,
            'roi': 0.0,
            'expected_value': 0.0
        }

    wins = sum(1 for b in bets if b['won'])
    losses = len(bets) - wins
    win_rate = wins / len(bets)

    # Standard -110 odds (bet $110 to win $100)
    # Win: +$100, Loss: -$110
    profit = (wins * 100) - (losses * 110)
    total_wagered = len(bets) * 110
    roi = profit / total_wagered

    return {
        'total_bets': len(bets),
        'wins': wins,
        'losses': losses,
        'win_rate': float(win_rate),
        'roi': float(roi),
        'expected_value': float(profit / len(bets)),  # Per bet
        'total_profit': float(profit),
        'assumptions': {
            'line': assumed_line,
            'edge_threshold': edge_threshold,
            'odds': '-110'
        }
    }


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Run backtest framework with advanced metric validation'
    )
    parser.add_argument('--season', type=int, default=2024,
                       help='Season year to backtest')
    parser.add_argument('--model', type=Path,
                       default=Path('outputs/models/passing_model.pkl'),
                       help='Path to trained model')
    parser.add_argument('--features', type=Path,
                       default=Path('outputs/player_pbp_features_by_id.json'),
                       help='Path to features file')
    parser.add_argument('--actuals', type=Path,
                       default=Path('inputs/player_stats_2024.csv'),
                       help='Path to actual outcomes CSV')
    parser.add_argument('--weeks', type=str, default=None,
                       help='Weeks to test (e.g., "1-8" or "1,5,10")')
    parser.add_argument('--output', type=Path,
                       default=Path('outputs/backtest_report.json'),
                       help='Path to save backtest report')
    args = parser.parse_args()

    # Parse weeks
    weeks = None
    if args.weeks:
        if '-' in args.weeks:
            start, end = map(int, args.weeks.split('-'))
            weeks = list(range(start, end + 1))
        elif ',' in args.weeks:
            weeks = [int(w) for w in args.weeks.split(',')]
        else:
            weeks = [int(args.weeks)]

    # Run backtest
    results = run_backtest(
        args.season,
        args.model,
        args.features,
        args.actuals,
        args.output,
        weeks
    )

    # Print summary
    if 'error' not in results:
        print(f"\n{'='*60}")
        print(f"âœ“ Backtest Complete!")
        print(f"{'='*60}")
        print(f"\nðŸ“Š Summary:")
        print(f"  Season: {results['season']}")
        print(f"  Weeks: {results['weeks_tested']}")
        print(f"  RMSE: {results['overall_metrics']['rmse']:.2f} yards")
        print(f"  MAE: {results['overall_metrics']['mae']:.2f} yards")
        print(f"  RÂ²: {results['overall_metrics']['r_squared']:.3f}")
        print(f"  Simulated ROI: {results['betting_simulation']['roi']:.2%}")
        print(f"\nðŸ’¾ Full report saved to: {args.output}")
    else:
        print(f"\nâš ï¸  Backtest failed: {results.get('error')}")
